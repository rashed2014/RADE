{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ecc7ce5-d130-4612-a87d-dc9691980cfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e8b7b3d-21ec-4637-a5ec-3d5176563a6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78bc2c1c-a29f-4ed5-b004-2016db860d3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from rade import RADE, RetrievedPage, DocumentPage\n",
    "from utils.azure_doc_intel import parse_azureDocIntell\n",
    "import time\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26fbebb6-0e89-455b-a979-e49bb1f9c480",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Initialize RADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8798d908-20e5-4124-98b5-f2079c4bb0ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# clear cache\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "# Instantiate RADE\n",
    "rade = RADE(use_flash_attention=False, \n",
    "            max_pages=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "750ea096-8bf6-40d0-b712-5d82dbadedac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Add and a index document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bb3163f-5aef-4848-aeae-d9f9b46c27c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#indexing method\n",
    "def add_index_document(pdf_file: str, model: RADE = rade, doc_id: int = 1):\n",
    "    \"\"\"\n",
    "    Add and index a document in RADE\n",
    "\n",
    "    Args:\n",
    "        pdf_file (str): Path to the PDF document.\n",
    "    \"\"\"\n",
    "    \n",
    "    model.add_document(pdf_file, doc_id)\n",
    "    model.build_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b314797d-2f37-47a8-9a2b-7fd48f8b6dcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Search on the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "becbafb7-f511-4113-ae60-b47e7db3a76b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def search_documents(queries: List[str], model: RADE = rade) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Search indexed documents using RAG.\n",
    "\n",
    "    Args:\n",
    "        query List[str]: Query strings.\n",
    "        model (RADE): RADE instance.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: Retrieved texts with metadata.\n",
    "    \"\"\"\n",
    "    search_result = rade.retrieve(queries)\n",
    "    assert len(search_result) == len(queries)\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "286012aa-1483-4255-8299-c9177452d106",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Canned queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf7cc628-a52b-4a16-95ba-ec9df1587acf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define entity queries and corresponding labels\n",
    "entity_queries = [\n",
    "    \"GRANTOR: what are the Grantors names? (Grantor)\",\n",
    "    \"TRUSTEE: what are the Trustees names? (Trustee)\",\n",
    "    \"Successor TRUSTEE: what are the Successor Trustee names (SUCCESSOR TRUSTEE)?\",\n",
    "    \"BENEFCIARIES: what are the beneficiaries names (BENEFICIARIES)?\",\n",
    "    \"SUCCESSOR BENEFCIARIES: what are the Successor beneficiaries names (SUCCESSOR Beneficiary)? \"\n",
    "]\n",
    "labels_list = [\n",
    "    [\"Grantor names\"],\n",
    "    [\"Trustee Names\"],\n",
    "    [\"Successor Trustee Names\"],\n",
    "    [\"Beneficiary names\"],\n",
    "    [\"Successor Beneficiary Names\"],\n",
    "    [\"Trust Name\"],\n",
    "    [\"Trust Date\"],\n",
    "    [\"Revocable\", \"Irevocable\"],\n",
    "    \n",
    "]\n",
    "qa_queries = [\n",
    "    \"What is the name of this trust?\",\n",
    "    \"What is the date of this trust?\",\n",
    "    \"Is this trust revocable or irrevocable?\",\n",
    "]\n",
    "all_queries_dict = {\n",
    "    \"entity_queries\": entity_queries,\n",
    "    \"qa_queries\": qa_queries\n",
    "}\n",
    "all_queries = all_queries_dict[\"entity_queries\"] + all_queries_dict[\"qa_queries\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9efd50b7-3fe4-4dce-9ac4-3fd6913cd7e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df854bd6-ff2e-4551-b96c-68456a9c55a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_document(pdf_files: [str], \n",
    "                     all_queries: List[str],\n",
    "                     model: RADE = rade) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process a document by indexing, retrieving, and extracting entities.\n",
    "\n",
    "    Args:\n",
    "        pdf_file (str): Path to the PDF document.\n",
    "        qa_questions (List[str]): List of questions for RoBERTa QA.\n",
    "        gliner_labels (List[str]): List of target labels for GLiNER.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing results, including time taken to process.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    search_result = search_documents(all_queries)\n",
    "    # Store results and parsed page contexts\n",
    "    qa_results = []\n",
    "    contexts_map = {}  # Dictionary to cache parsed pages\n",
    "\n",
    "    for query_idx in tqdm(range(len(all_queries)), desc=\"Processing QA Queries\"):\n",
    "        query = all_queries[query_idx]\n",
    "        pages = search_result[query_idx]\n",
    "\n",
    "        all_pages = []\n",
    "        retrieved_page_nums = []\n",
    "        for page in pages:\n",
    "            page_num = page.page.page_num\n",
    "            retrieved_page_nums.append(str(page_num))\n",
    "\n",
    "            # Check if page has been parsed before\n",
    "            if page_num in contexts_map:\n",
    "                parsed = contexts_map[page_num]\n",
    "                print(f\"Using cached parsed result for page: {page_num}\")\n",
    "            else:\n",
    "                print(f\"Parsing new page: {page_num}\")\n",
    "                parsed = parse_azureDocIntell(page.page.image)\n",
    "                contexts_map[page_num] = parsed\n",
    "\n",
    "            all_pages.append(parsed)\n",
    "\n",
    "        # Combine parsed pages into a single string\n",
    "        combined_pages = \" \".join(all_pages) if all_pages else \"\"\n",
    "        page_nums = \" \".join(retrieved_page_nums) if retrieved_page_nums else \"\"\n",
    "\n",
    "        # print(f\"retrived pages {page_nums} \")\n",
    "        # Run QA model to extract answer\n",
    "        qa_answer = rade.run_qa_pipeline(query, combined_pages)\n",
    "\n",
    "        # Store result\n",
    "        qa_results.append({\n",
    "            \"query\": query,\n",
    "            \"retrieved_pages\": page_nums,\n",
    "            \"context\": combined_pages,\n",
    "            \"RoBerta Answer\": qa_answer\n",
    "        })\n",
    "\n",
    "    entity_results = []\n",
    "    for query_idx in tqdm(range(len(all_queries)), desc=\"Processing Entity Queries\"):\n",
    "        query = all_queries[query_idx]\n",
    "        labels = labels_list[query_idx]\n",
    "        pages = search_result[query_idx]\n",
    "\n",
    "        all_pages = []\n",
    "        retrieved_page_nums = []\n",
    "        for page in pages:\n",
    "            page_num = page.page.page_num\n",
    "            retrieved_page_nums.append(str(page_num))\n",
    "            # Check if page has been parsed before\n",
    "            if page_num in contexts_map:\n",
    "                parsed = contexts_map[page_num]\n",
    "                print(f\"Using cached parsed result for page: {page_num}\")\n",
    "            else:\n",
    "                print(f\"Parsing new page: {page_num}\")\n",
    "                parsed = parse_azureDocIntell(page.page.image)\n",
    "                contexts_map[page_num] = parsed\n",
    "\n",
    "            all_pages.append(parsed)\n",
    "\n",
    "        # Combine parsed pages into a single string\n",
    "        combined_pages = \" \".join(all_pages) if all_pages else \"\"\n",
    "        \n",
    "        page_nums = \" \".join(retrieved_page_nums) if retrieved_page_nums else \"\"\n",
    "\n",
    "\n",
    "        # Extract entities from parsed text if only single gpu\n",
    "        entities = rade.extract_entities_with_gliner(combined_pages, labels)\n",
    "\n",
    "        # Store result\n",
    "        entity_results.append({\n",
    "            \"query\": query,\n",
    "            \"retrieved_pages\": page_nums,\n",
    "            \"context\": combined_pages,\n",
    "            \"GLiNER Answer\": entities\n",
    "        })\n",
    "\n",
    "    # Convert lists of dictionaries to pandas DataFrames\n",
    "    df_entities = pd.DataFrame(entity_results)\n",
    "    df_qa = pd.DataFrame(qa_results)\n",
    "\n",
    "    # Perform the merge and prioritize 'context' from entity_results\n",
    "    combined_df = pd.merge(\n",
    "        df_entities.drop(columns='context', errors='ignore'),  # Remove 'context' to avoid conflict\n",
    "        df_qa,\n",
    "        on='query',\n",
    "        suffixes=('_gliner', '_qa'),\n",
    "        how='outer'\n",
    "    )\n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44d1bc15-ac91-497a-9736-d4511047e112",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Main method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a100f1b5-1be3-43f2-8089-e47da01c28aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to process multiple documents and save results.\n",
    "    \"\"\"\n",
    "    # Directory containing PDF files\n",
    "    pdf_dir = \"/Workspace/Shared/pdf-etl-ocr-inference/SampleTrustDocs/pdf\" \n",
    "    pdf_files = [os.path.join(pdf_dir, f) for f in os.listdir(pdf_dir) if f.endswith(\".pdf\")]\n",
    "\n",
    "    if not pdf_files:\n",
    "        print(f\"No PDF files found in directory: {pdf_dir}\")\n",
    "        \n",
    "    pdf_path = pdf_files[2]#test a single file\n",
    "\n",
    "    #combine queries\n",
    "    all_queries = all_queries_dict[\"entity_queries\"] + all_queries_dict[\"qa_queries\"]\n",
    "    #add and index the document\n",
    "    add_index_document(pdf_path)\n",
    "    df = process_document(pdf_path, all_queries)\n",
    "    output_dir = \"output\"\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Ensure the output directory exists\n",
    "\n",
    "    # Generate the filename based on the PDF file basename\n",
    "    output_filename = os.path.splitext(os.path.basename(pdf_path))[0] + \"_results.csv\"\n",
    "\n",
    "    # Save the combined DataFrame to the output directory\n",
    "    output_file_path = os.path.join(output_dir, output_filename)\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"Combined DataFrame saved to: {output_file_path}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1b6f6be-6df2-4f95-b329-0e7ee1234de6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.perf_counter()  # Start timing\n",
    "main()  # Run main function\n",
    "end_time = time.perf_counter()  # End timing\n",
    "\n",
    "print(f\"Processing completed.\\nExecution Time: {end_time - start_time:.4f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "RADE2Search-notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
